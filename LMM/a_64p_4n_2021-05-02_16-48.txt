Command:		mpirun -np 64 ./a.out
Resources:		4 nodes (16 physical, 16 logical cores per node)
Memory:			126 GiB per node
Tasks:			64 processes
Machine:		hpc-class16
Started on:		Sun May 2 16:48:43 2021
Total time:		4 seconds (0 minutes)
Executable:		a.out
Full path:		/home/yhkwon/LMM
Input file:		
Notes:			

Summary: a.out is Compute-bound in this configuration
Compute:				          85.0% |========|
MPI:					          15.0% ||
I/O:					           0.0% |
This application run was Compute-bound. A breakdown of this time and advice for investigating further is found in the CPU section below.
As very little time is spent in MPI calls, this code may also benefit from running at larger scales.

CPU:
A breakdown of the 85.0% total compute time:
Scalar numeric ops:			          38.7% |===|
Vector numeric ops:			          17.4% |=|
Memory accesses:			          40.9% |===|
The per-core performance is arithmetic-bound. Try to increase the amount of time spent in vectorized instructions by analyzing the compiler's vectorization reports.
Significant time is spent on memory accesses. Use a profiler to identify time-consuming loops and check their cache performance.

Threads:
A breakdown of how multiple threads were used:
Computation:				           0.0% |
Synchronization:			           0.0% |
Physical core utilization:		         100.0% |=========|
System load:				         103.2% |=========|
No measurable time is spent in multithreaded code.


MPI:
A breakdown of the 15.0% MPI time:
Time in collective calls:		         100.0% |=========|
Time in point-to-point calls:		           0.0% |
Effective process collective rate:	           62.4 bytes/s
Effective process point-to-point rate:	              0 bytes/s
Most of the time is spent in collective calls with a very low transfer rate. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.


I/O:
A breakdown of the 0.0% I/O time:
Time in reads:				           0.0% |
Time in writes:				           0.0% |
Effective process read rate:		              0 bytes/s
Effective process write rate:		              0 bytes/s
No time is spent in I/O operations. There's nothing to optimize here!


Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:		        1.3e+08 bytes
Peak process memory usage:		       1.34e+08 bytes
Peak node memory usage:			           5.0% ||
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.


Energy:
A breakdown of how the 415 mWh was used:
CPU:					         100.0% |=========|
System:					  not supported 
Mean node power:			  not supported W
Peak node power:			  not supported W
The whole system energy has been calculated using the CPU energy usage.
No Allinea IPMI Energy Agent config file found in (null).
Did you start the Allinea IPMI Energy Agent?
